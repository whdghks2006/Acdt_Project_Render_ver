# -*- coding: utf-8 -*-
"""
Train NER Model with Augmented Data (v2)
Uses the combined training data to train improved NER model.

Training Configuration:
- Base model: en_core_web_lg
- Labels: START_DATE, START_TIME, END_DATE, END_TIME, LOC, EVENT_TITLE
- Epochs: 50
- Dropout: 0.25
"""

import spacy
from spacy.training.example import Example
from spacy.util import minibatch, compounding
import json
import random
import warnings
import os

warnings.filterwarnings("ignore")

# ==============================================================================
# Configuration
# ==============================================================================

TRAIN_FILE = "train_data_combined.json"  # Generated by merge_training_data.py
DEV_FILE = "_dev_backup/test_data.json"
OUTPUT_DIR = "./output/new_ner_model_v2"
BASE_MODEL = "en_core_web_lg"

# Training hyperparameters (v2 config)
N_ITER = 50
DROPOUT = 0.25
BATCH_SIZE = 32

# Entity labels
LABELS = ["START_DATE", "START_TIME", "END_DATE", "END_TIME", "LOC", "EVENT_TITLE"]

# Field to label mapping
FIELD_TO_LABEL = {
    "Date_Entity": "START_DATE",
    "Time_Entity": "START_TIME",
    "End_Date_Entity": "END_DATE",
    "End_Time_Entity": "END_TIME",
    "Location_Entity": "LOC",
    "Event_Entity": "EVENT_TITLE"
}

# ==============================================================================
# Data Loading & Conversion
# ==============================================================================

def load_and_convert_data(filepath):
    """Load JSON and convert to spaCy format"""
    with open(filepath, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    
    print(f"[INFO] Raw data loaded: {len(raw_data)} items")
    
    converted = []
    skipped = 0
    
    for item in raw_data:
        text = item.get("Text", "").strip()
        if not text:
            skipped += 1
            continue
        
        entities = []
        for field, label in FIELD_TO_LABEL.items():
            value = item.get(field, "")
            if value:
                value = str(value).strip()
                start = text.find(value)
                if start != -1:
                    end = start + len(value)
                    entities.append((start, end, label))
        
        if entities:
            converted.append((text, {"entities": entities}))
        else:
            # Include texts without entities too (negative examples)
            converted.append((text, {"entities": []}))
    
    print(f"[INFO] Conversion complete: {len(converted)} items")
    print(f"[INFO] Skipped: {skipped} items")
    
    return converted

# ==============================================================================
# Entity Alignment
# ==============================================================================

def align_entities_to_tokens(nlp, data):
    """Align entity spans to token boundaries"""
    aligned_data = []
    auto_fixed = 0
    skipped = 0
    
    for text, annot in data:
        try:
            doc = nlp.make_doc(text)
            new_entities = []
            
            for start, end, label in annot.get("entities", []):
                span = doc.char_span(start, end, label=label, alignment_mode="expand")
                if span:
                    new_entities.append((span.start_char, span.end_char, label))
                    if (span.start_char, span.end_char) != (start, end):
                        auto_fixed += 1
                else:
                    skipped += 1
            
            aligned_data.append((text, {"entities": new_entities}))
        except Exception as e:
            skipped += 1
    
    print(f"[INFO] Token alignment complete")
    print(f"   - Auto-fixed: {auto_fixed}")
    print(f"   - Skipped: {skipped}")
    print(f"   - Final training data: {len(aligned_data)}")
    
    return aligned_data

# ==============================================================================
# Training
# ==============================================================================

def train_model(train_data, dev_data):
    """Train NER model"""
    
    # Load base model
    print(f"\n[Step 2] Loading base model ({BASE_MODEL})...")
    nlp = spacy.load(BASE_MODEL)
    print(f"[INFO] '{BASE_MODEL}' loaded successfully")
    
    # Align entities
    print("\n[Step 3] Aligning to token boundaries...")
    train_data = align_entities_to_tokens(nlp, train_data)
    dev_data = align_entities_to_tokens(nlp, dev_data)
    
    print(f"\n[Step 4] Data summary...")
    print(f"   - Train data: {len(train_data)}")
    print(f"   - Dev data: {len(dev_data)}")
    
    # Get NER pipeline
    if "ner" not in nlp.pipe_names:
        ner = nlp.add_pipe("ner", last=True)
    else:
        ner = nlp.get_pipe("ner")
    
    # Add labels
    print(f"\n[Step 5] Configuring NER pipeline...")
    print(f"   - Labels: {LABELS}")
    for label in LABELS:
        ner.add_label(label)
    
    # Convert to Examples
    train_examples = []
    for text, annot in train_data:
        try:
            doc = nlp.make_doc(text)
            example = Example.from_dict(doc, annot)
            train_examples.append(example)
        except:
            pass
    
    # Training
    print(f"\n[Step 6] Starting training...")
    print(f"   - Epochs: {N_ITER}")
    print(f"   - Dropout: {DROPOUT}")
    print("-" * 60)
    
    # Disable other pipes during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "ner"]
    
    with nlp.disable_pipes(*other_pipes):
        optimizer = nlp.resume_training()
        
        best_f1 = 0
        best_epoch = 0
        
        for epoch in range(N_ITER):
            random.shuffle(train_examples)
            losses = {}
            
            # Mini-batch training
            batches = minibatch(train_examples, size=compounding(4., BATCH_SIZE, 1.001))
            for batch in batches:
                nlp.update(batch, drop=DROPOUT, losses=losses, sgd=optimizer)
            
            # Log progress
            loss_val = losses.get("ner", 0)
            
            # Evaluate every 5 epochs
            if (epoch + 1) % 5 == 0:
                # Simple evaluation on dev set
                dev_examples = []
                for text, annot in dev_data:
                    doc = nlp.make_doc(text)
                    dev_examples.append(Example.from_dict(doc, annot))
                
                try:
                    from spacy.scorer import Scorer
                    scorer = Scorer()
                    scores = scorer.score(dev_examples)
                    
                    p = scores.get("ents_p", 0) * 100
                    r = scores.get("ents_r", 0) * 100
                    f1 = scores.get("ents_f", 0) * 100
                    
                    print(f"[Epoch {epoch+1:2d}/{N_ITER}] Loss: {loss_val:8.2f} | P: {p:.1f}% R: {r:.1f}% F1: {f1:.1f}%")
                    
                    if f1 > best_f1:
                        best_f1 = f1
                        best_epoch = epoch + 1
                except:
                    print(f"[Epoch {epoch+1:2d}/{N_ITER}] Loss: {loss_val:8.2f}")
            else:
                print(f"[Epoch {epoch+1:2d}/{N_ITER}] Loss: {loss_val:8.2f}")
    
    print("-" * 60)
    print(f"[BEST] F1: {best_f1:.1f}% (Epoch {best_epoch})")
    
    return nlp

# ==============================================================================
# Main Execution
# ==============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("NER Model Training (Augmented Data Version)")
    print("=" * 60)
    
    print("\n[Step 0] Configuration...")
    print(f"   - Output: {OUTPUT_DIR}")
    
    # Load data
    print("\n[Step 1] Loading and converting data...")
    train_data = load_and_convert_data(TRAIN_FILE)
    dev_data = load_and_convert_data(DEV_FILE)
    
    # Train
    nlp = train_model(train_data, dev_data)
    
    # Save model
    print("\n[Step 7] Saving model...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    nlp.to_disk(OUTPUT_DIR)
    print(f"[INFO] Model saved to: {OUTPUT_DIR}")
    
    # Test
    print("\n[Step 8] Testing...")
    print("-" * 60)
    
    test_sentences = [
        "Team meeting from 2 PM to 4 PM tomorrow at Conference Room A.",
        "The workshop runs from January 15th through January 20th.",
        "Dinner party at Sarah's house this Saturday at 7 PM.",
        "I'll be available between 10 AM and noon for calls.",
        "Lunch with Mom at Starbucks.",
        "Gym at 6pm.",
        "Coffee with Sarah tomorrow 10am.",
        "Quick meeting.",
    ]
    
    for sentence in test_sentences:
        doc = nlp(sentence)
        print(f'\nInput: "{sentence}"')
        for ent in doc.ents:
            print(f"   - {ent.label_}: \"{ent.text}\"")
        if not doc.ents:
            print("   - (No entities detected)")
    
    print("\n" + "=" * 60)
    print("Training Complete!")
    print(f"Output: {OUTPUT_DIR}")
    print("=" * 60)

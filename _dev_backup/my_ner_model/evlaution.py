import spacy
import json
import os
import warnings
from spacy.training.example import Example
from spacy.scorer import Scorer

# ==============================================================================
# Configuration / ì„¤ì •
# ==============================================================================

# Path to the saved model
# ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ
MODEL_DIR = r"C:/Users/82105/PycharmProjects/Acdt Project 2/my_ner_model"

# Path to the test data file
# í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
TEST_FILE = r"C:/Users/82105/PycharmProjects/Acdt Project 2/test.json"


# ==============================================================================
# Helper Function / í—¬í¼ í•¨ìˆ˜ (Training ì½”ë“œì™€ ë™ì¼)
# ==============================================================================

def clean_and_fix_data(nlp, data):
    """
    Aligns entities with token boundaries using 'expand' mode.
    í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ í•™ìŠµ ë•Œì™€ ë˜‘ê°™ì´ ì—”í‹°í‹° ë²”ìœ„ë¥¼ í† í°ì— ë§ì¶° ìˆ˜ì •í•©ë‹ˆë‹¤.
    """
    clean_data_list = []
    fixed_count = 0

    for text, annotations in data:
        doc = nlp.make_doc(text)
        valid_ents = []

        if "entities" not in annotations:
            continue

        for start, end, label in annotations["entities"]:
            # 1. Strict alignment check
            span = doc.char_span(start, end, label=label)

            # 2. If failed, try expand alignment
            if span is None:
                span = doc.char_span(start, end, label=label, alignment_mode="expand")
                if span is not None:
                    fixed_count += 1

            if span is not None:
                valid_ents.append(span)

        # Convert spans back to (start, end, label) tuples
        final_ents = [(e.start_char, e.end_char, e.label_) for e in valid_ents]

        # Add only if valid entities exist
        if final_ents:
            # Remove duplicates just in case
            final_ents = list(set(final_ents))
            clean_data_list.append((text, {"entities": final_ents}))

    print(f"ğŸ§¹ Test Data cleaned.")
    print(f"   - Fixed/Aligned {fixed_count} entities.")
    print(f"   - Valid examples ready for evaluation: {len(clean_data_list)}")
    return clean_data_list


# ==============================================================================
# Main Execution / ë©”ì¸ ì‹¤í–‰
# ==============================================================================

if __name__ == "__main__":
    # Ignore alignment warnings
    warnings.filterwarnings("ignore")

    print("--------------------------------------------------")
    print("ğŸ”„ Step 1: Loading Model and Test Data...")

    # 1. Load Model
    if not os.path.exists(MODEL_DIR):
        print(f"âŒ Error: Model not found at {MODEL_DIR}")
        exit()

    try:
        nlp = spacy.load(MODEL_DIR)
        print(f"âœ… Model loaded from: {MODEL_DIR}")
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        exit()

    # 2. Load Test Data
    if not os.path.exists(TEST_FILE):
        print(f"âŒ Error: Test file not found at {TEST_FILE}")
        exit()

    with open(TEST_FILE, 'r', encoding='utf-8') as f:
        RAW_TEST_DATA = json.load(f)
    print(f"âœ… Raw Test data loaded: {len(RAW_TEST_DATA)} examples")

    print("\nğŸ”„ Step 2: Preparing Data...")
    # Clean the test data using the loaded model's tokenizer
    # ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ 'ìŠ¤ë§ˆíŠ¸ ë³´ì •' ìˆ˜í–‰
    TEST_DATA = clean_and_fix_data(nlp, RAW_TEST_DATA)

    print("\nğŸ”„ Step 3: Evaluating Performance...")

    examples = []
    for text, annotations in TEST_DATA:
        doc = nlp.make_doc(text)
        try:
            example = Example.from_dict(doc, annotations)
            examples.append(example)
        except Exception:
            continue

    # Scoring
    scores = nlp.evaluate(examples)

    print("\n--------------------------------------------------")
    print("ğŸ“Š EVALUATION RESULTS (KPIs)")
    print("--------------------------------------------------")

    precision = scores.get('ents_p', 0.0)
    recall = scores.get('ents_r', 0.0)
    f1_score = scores.get('ents_f', 0.0)

    print(f"ğŸ† Overall Precision : {precision:.2%}")
    print(f"ğŸ† Overall Recall    : {recall:.2%}")
    print(f"ğŸ† Overall F1-Score  : {f1_score:.2%}  <-- This is your main KPI")

    print("\n--------------------------------------------------")
    print("ğŸ” Breakdown by Entity Type:")

    per_ents = scores.get('ents_per_type', {})

    # Sort labels for cleaner output
    sorted_labels = sorted(per_ents.keys())

    if not sorted_labels:
        print("   (No entities detected)")

    for label in sorted_labels:
        metrics = per_ents[label]
        p = metrics.get('p', 0.0)
        r = metrics.get('r', 0.0)
        f = metrics.get('f', 0.0)
        print(f"   - {label:<6} | F1-Score: {f:.2%} (P: {p:.2f}, R: {r:.2f})")

    print("--------------------------------------------------")
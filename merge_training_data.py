# -*- coding: utf-8 -*-
"""
Merge Training Data (v2)
Combines multiple training data sources into a single file for v2 training.
"""

import json
import os

# ==============================================================================
# Configuration
# ==============================================================================

OUTPUT_FILE = "train_data_combined.json"

# List of training data files to merge
DATA_FILES = [
    "_dev_backup/train_data.json",
    "_dev_backup/final_training_data.json",
    "_dev_backup/synthetic_training_data.json",
    "_dev_backup/new_ai_data.json",
    "augmented_training_data.json",  # Generated by generate_augmented_data.py
]

# ==============================================================================
# Data Merging Functions
# ==============================================================================

def load_json_file(filepath):
    """Load JSON file and return as list"""
    if not os.path.exists(filepath):
        print(f"[SKIP] File not found: {filepath}")
        return []
    
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"[LOAD] {filepath}: {len(data)} items")
    return data

def normalize_item(item):
    """Normalize item to consistent format"""
    # Handle different key names
    text = item.get("Text") or item.get("text") or ""
    
    return {
        "Text": text,
        "Date_Entity": item.get("Date_Entity", "") or "",
        "Time_Entity": item.get("Time_Entity", "") or "",
        "End_Date_Entity": item.get("End_Date_Entity", "") or "",
        "End_Time_Entity": item.get("End_Time_Entity", "") or "",
        "Location_Entity": item.get("Location_Entity", "") or "",
        "Event_Entity": item.get("Event_Entity", "") or "",
    }

def merge_all_data(file_list):
    """Merge all data files"""
    all_data = []
    
    for filepath in file_list:
        data = load_json_file(filepath)
        for item in data:
            normalized = normalize_item(item)
            if normalized["Text"]:  # Only add if text exists
                all_data.append(normalized)
    
    print(f"\n[TOTAL] Before dedup: {len(all_data)} items")
    
    # Remove duplicates based on Text
    seen_texts = set()
    unique_data = []
    for item in all_data:
        text_key = item["Text"].strip().lower()
        if text_key not in seen_texts:
            seen_texts.add(text_key)
            unique_data.append(item)
    
    print(f"[UNIQUE] After dedup: {len(unique_data)} items")
    
    return unique_data

# ==============================================================================
# Main Execution
# ==============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("Training Data Merger (v2)")
    print("=" * 60)
    
    # Merge all data
    merged_data = merge_all_data(DATA_FILES)
    
    # Save to file
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    print(f"\n[SAVED] {OUTPUT_FILE}: {len(merged_data)} items")
    
    # Summary by entity type
    print("\n" + "=" * 60)
    print("Entity distribution:")
    print("=" * 60)
    
    counts = {
        "Date": sum(1 for d in merged_data if d["Date_Entity"]),
        "Time": sum(1 for d in merged_data if d["Time_Entity"]),
        "End_Date": sum(1 for d in merged_data if d["End_Date_Entity"]),
        "End_Time": sum(1 for d in merged_data if d["End_Time_Entity"]),
        "Location": sum(1 for d in merged_data if d["Location_Entity"]),
        "Event": sum(1 for d in merged_data if d["Event_Entity"]),
    }
    
    for entity, count in counts.items():
        print(f"  {entity}: {count} ({count/len(merged_data)*100:.1f}%)")
    
    print("=" * 60)
    print("Done!")
    print("=" * 60)
